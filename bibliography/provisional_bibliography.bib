@Article{HendrycksDan2019BNNR,
  author    = {Hendrycks, Dan and Dietterich, Thomas},
  title     = {Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
  year      = {2019},
  abstract  = {In this paper we establish rigorous benchmarks for image classifier
robustness. Our first benchmark, ImageNet-C, standardizes and expands the
corruption robustness topic, while showing which classifiers are preferable in
safety-critical applications. Then we propose a new dataset called ImageNet-P
which enables researchers to benchmark a classifier's robustness to common
perturbations. Unlike recent robustness research, this benchmark evaluates
performance on common corruptions and perturbations not worst-case adversarial
perturbations. We find that there are negligible changes in relative corruption
robustness from AlexNet classifiers to ResNet classifiers. Afterward we
discover ways to enhance corruption and perturbation robustness. We even find
that a bypassed adversarial defense provides substantial common perturbation
robustness. Together our benchmarks may aid future work toward networks that
robustly generalize.},
  copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition ; Computer Science - Learning ; Statistics - Machine Learning},
  language  = {eng},
  ranking   = {rank4},
}

@Article{WangYue2022Aiis,
  author    = {Wang, Yue and Chung, Sai Ho},
  journal   = {Industrial management + data systems},
  title     = {Artificial intelligence in safety-critical systems: a systematic review},
  year      = {2022},
  issn      = {0263-5577},
  number    = {2},
  pages     = {442-470},
  volume    = {122},
  abstract  = {PurposeThis study is a systematic literature review of the application of artificial intelligence (AI) in safety-critical systems. The authors aim to present the current application status according to different AI techniques and propose some research directions and insights to promote its wider application.Design/methodology/approachA total of 92 articles were selected for this review through a systematic literature review along with a thematic analysis.FindingsThe literature is divided into three themes: interpretable method, explain model behavior and reinforcement of safe learning. Among AI techniques, the most widely used are Bayesian networks (BNs) and deep neural networks. In addition, given the huge potential in this field, four future research directions were also proposed.Practical implicationsThis study is of vital interest to industry practitioners and regulators in safety-critical domain, as it provided a clear picture of the current status and pointed out that some AI techniques have great application potential. For those that are inherently appropriate for use in safety-critical systems, regulators can conduct in-depth studies to validate and encourage their use in the industry.Originality/valueThis is the first review of the application of AI in safety-critical systems in the literature. It marks the first step toward advancing AI in safety-critical domain. The paper has potential values to promote the use of the term “safety-critical” and to improve the phenomenon of literature fragmentation.},
  address   = {Leeds},
  copyright = {Emerald Publishing Limited},
  keywords  = {Adversarial examples ; Artificial intelligence ; Artificial neural networks ; Bayesian ; Bayesian analysis ; Computer Science ; Computer Science, Interdisciplinary Applications ; Decision making ; Domains ; Embedded systems ; Engineering ; Engineering, Industrial ; Failure ; Formal verification ; Literature reviews ; Machine learning ; Neural network ; Neural networks ; Nuclear power plants ; Safety critical ; Safety-critical system ; Science & Technology ; Software ; Systematic review ; Technology ; Trust},
  language  = {eng},
  publisher = {Emerald Publishing Limited},
  ranking   = {rank4},
}

@Article{Chih-HongCheng2018RMNA,
  author    = {Chih-Hong, Cheng and Nührenberg, Georg and Yasuoka, Hirotoshi},
  journal   = {arXiv.org},
  title     = {Runtime Monitoring Neuron Activation Patterns},
  year      = {2018},
  issn      = {2331-8422},
  abstract  = {For using neural networks in safety critical domains, it is important to know if a decision made by a neural network is supported by prior similarities in training. We propose runtime neuron activation pattern monitoring - after the standard training process, one creates a monitor by feeding the training data to the network again in order to store the neuron activation patterns in abstract form. In operation, a classification decision over an input is further supplemented by examining if a pattern similar (measured by Hamming distance) to the generated pattern is contained in the monitor. If the monitor does not contain any pattern similar to the generated pattern, it raises a warning that the decision is not based on the training data. Our experiments show that, by adjusting the similarity-threshold for activation patterns, the monitors can report a significant portion of misclassfications to be not supported by training with a small false-positive rate, when evaluated on a test set.},
  address   = {Ithaca},
  copyright = {2018. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  keywords  = {Activation ; Computer Science - Learning ; Domains ; Monitoring ; Neural networks ; Run time (computers) ; Safety critical ; Statistics - Machine Learning ; Training},
  language  = {eng},
  publisher = {Cornell University Library, arXiv.org},
}

@Misc{matthias_de_ryck_robot_demonstrator,
  author       = {Matthias De Ryck},
  howpublished = {\url{https://github.com/MatthiasDR96/robot_demonstrator.git}},
  note         = {GitHub repository},
  title        = {Robot Demonstrator Code},
  year         = {2023},
  ranking      = {rank5},
}

@Article{LakshminarayananBalaji2016SaSP,
  author    = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  journal   = {arXiv.org},
  title     = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
  year      = {2016},
  issn      = {2331-8422},
  abstract  = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
  address   = {Ithaca},
  copyright = {2017. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  keywords  = {Bayesian analysis ; Black boxes ; Computer Science - Learning ; Current distribution ; Estimates ; Neural networks ; Parallel processing ; Statistics - Machine Learning ; Uncertainty},
  language  = {eng},
  publisher = {Cornell University Library, arXiv.org},
}

@Article{SahuAmit2020AotN,
  author    = {Sahu, Amit and Vállez, Noelia and Rodríguez-Bobada, Rosana and Alhaddad, Mohamad and Moured, Omar and Neugschwandtner, Georg},
  journal   = {arXiv (Cornell University)},
  title     = {Application of the Neural Network Dependability Kit in Real-World Environments},
  year      = {2020},
  issn      = {2331-8422},
  abstract  = {In this paper, we provide a guideline for using the Neural Network Dependability Kit (NNDK) during the development process of NN models, and show how the algorithm is applied in two image classification use cases. The case studies demonstrate the usage of the dependability kit to obtain insights about the NN model and how they informed the development process of the neural network model. After interpreting neural networks via the different metrics available in the NNDK, the developers were able to increase the NNs' accuracy, trust the developed networks, and make them more robust. In addition, we obtained a novel application-oriented technique to provide supporting evidence for an NN's classification result to the user. In the medical image classification use case, it was used to retrieve case images from the training dataset that were similar to the current patient's image and could therefore act as a support for the NN model's decision and aid doctors in interpreting the results.},
  address   = {Ithaca},
  copyright = {2020. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  keywords  = {Algorithms ; Computer Science - Learning ; Computer Science - Software Engineering ; Image classification ; Medical imaging ; Neural networks ; Physicians},
  language  = {eng},
  publisher = {Cornell University Library, arXiv.org},
}

@InCollection{FrangiAlejandroF2015UCNf,
  author    = {Frangi, Alejandro F and Hornegger, Joachim and Navab, Nassir and Wells, William M},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing AG},
  title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  year      = {2015},
  address   = {Switzerland},
  isbn      = {9783319245737},
  pages     = {234-241},
  series    = {Lecture Notes in Computer Science},
  volume    = {9351},
  abstract  = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  copyright = {Springer International Publishing Switzerland 2015},
  issn      = {0302-9743},
  keywords  = {Artificial intelligence ; Artificial neural network ; Brain segmentation ; Computer science ; Computer vision ; Context (language use) ; Convolutional Layer ; Data Augmentation ; Deep learning ; Deep Network ; Graphics programming ; Ground Truth Segmentation ; Image processing ; Image translation ; Pattern recognition ; Segmentation ; Training Image},
  language  = {eng},
  ranking   = {rank5},
}

@Comment{jabref-meta: databaseType:bibtex;}
