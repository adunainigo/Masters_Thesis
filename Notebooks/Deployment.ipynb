{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.- Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create --name thesis_inigo python=3.8.19\n",
    "! conda activate thesis_inigo\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Modification in the Robot Demonstrator Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the modification I applied in the [MatthiasDeRyck Robot Demonstrator](https://github.com/MatthiasDR96/robot_demonstrator.git) in the file \"./robot_demonstrator/scripts/main_threaded.py\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Directorio que quieres añadir\n",
    "new_path = \"C:\\\\Users\\\\aduna\\\\Documents\\\\Master_KU_Leuven\\\\Master_Thesis\\\\program\\\\data\\\\Github Matthias\\\\robot_demonstrator\\\\src\"\n",
    "\n",
    "# Añadir el directorio al PYTHONPATH\n",
    "if new_path not in sys.path:\n",
    "    sys.path.append(new_path)\n",
    "import _thread\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from robot_demonstrator.ABB_IRB1200 import ABB_IRB1200\n",
    "from robot_demonstrator.Camera import *\n",
    "from robot_demonstrator.image_processing import *\n",
    "from robot_demonstrator.plot import *\n",
    "from robot_demonstrator.transformations import *\n",
    "from skimage import feature, measure, morphology\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create camera object\n",
    "cam = Camera()\n",
    "\n",
    "# Start camera\n",
    "cam.start()\n",
    "\n",
    "# Create camera object (online)\n",
    "robot = ABB_IRB1200(\"192.168.125.1\")\n",
    "\n",
    "# Start robot\n",
    "robot.start()\n",
    "\n",
    "# Load T_bc (Transformation matrix from robot base frame to camera frame)\n",
    "T_bc = np.load('./data/T_bc.npy')\n",
    "\n",
    "# Load perspective matrix (calculated using the image_rectification_test.py file)\n",
    "M = np.load('./data/perspective_transform.npy')\n",
    "\n",
    "# Load error model\n",
    "model = pickle.load(open('./data/error_model.sav', 'rb'))\n",
    "\n",
    "# Define pick and place orientation\n",
    "quat = list(quat_from_r(np.array([[-1, 0, 0], [0, 1, 0], [0, 0, -1]]))) # Quaternion of the pick and place orientation\n",
    "quat = [quat[3], quat[0], quat[1], quat[2]] # Convert [x y z w] to [w x y z]\n",
    "\n",
    "# Define fixed z-position to pick\n",
    "grip_height = 7\n",
    "\n",
    "# Define place position\n",
    "pose_place = [450.0, 290.0, 220] \n",
    "\n",
    "# Define offsets\n",
    "offset1 = np.array([0, 0, 40]) # Offset above the pick and place poses\n",
    "tool_offset = np.array([-math.sqrt(200), -math.sqrt(200), 170]) # Tool offset (Translation from robot end effector to TCP)\n",
    "\n",
    "# Define error\n",
    "error = [0, 0, 0] # [7, 3, 0]  Error in system obtained from data collection --> to be reduced\n",
    "\n",
    "# Robot boundaries\n",
    "xmin = 350 # Minimal Cartesian x-position\n",
    "xmax = 630 # Maximal Cartesian x-position\n",
    "ymin = -250 # Minimal Cartesian y-position\n",
    "ymax = 250 # Maximal Cartesian y-position\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Global params\n",
    "global xyz_base\n",
    "xyz_base = []\n",
    "\n",
    "# Robot task thread function\n",
    "def robot_task(name):\n",
    "\n",
    "\t# Define global variable\n",
    "\tglobal xyz_base\n",
    "\n",
    "\t# Loop\n",
    "\twhile True:\n",
    "\n",
    "\t\t# Sleep\n",
    "\t\ttime.sleep(1)\n",
    "\n",
    "\t\t# Get feasible positions\n",
    "\t\txyz_base_feasible = [xyz for xyz in xyz_base if not (xyz[0] > xmax or xyz[0] < xmin or xyz[1] > ymax or xyz[1] < ymin)]\n",
    "\n",
    "\t\t# Check if there are feasible positions\n",
    "\t\tif len(xyz_base_feasible) < 1: continue\n",
    "\n",
    "\t\t# Get first element\n",
    "\t\txyz_base_tmp = xyz_base_feasible[0]\n",
    "\n",
    "\t\t# Debug\n",
    "\t\tprint(\"\\nRobot - Start picking object!\\n\")\n",
    "\n",
    "\t\t# Set pick pose upper\n",
    "\t\trobot.con.set_cartesian([xyz_base_tmp + offset1, quat])\n",
    "\t\ttime.sleep(1)\n",
    "\n",
    "\t\t# Set pick pose\n",
    "\t\trobot.con.set_cartesian([xyz_base_tmp, quat])\n",
    "\t\ttime.sleep(1)\n",
    "\n",
    "\t\t# Set pick DIO\n",
    "\t\trobot.con.set_dio(1)\n",
    "\t\ttime.sleep(1)\n",
    "\n",
    "\t\t# Set pick pose upper\n",
    "\t\trobot.con.set_cartesian([xyz_base_tmp + offset1, quat])\n",
    "\t\ttime.sleep(1)\n",
    "\n",
    "\t\t# Set place pose upper\n",
    "\t\trobot.con.set_cartesian([pose_place + offset1, quat])\n",
    "\t\ttime.sleep(1)\n",
    "\n",
    "\t\t# Set place pose\n",
    "\t\trobot.con.set_cartesian([pose_place, quat])\n",
    "\t\ttime.sleep(1)\n",
    "\n",
    "\t\t# Set place DIO\n",
    "\t\trobot.con.set_dio(0)\n",
    "\t\ttime.sleep(1)\n",
    "\n",
    "\t\t# Set home position\n",
    "\t\trobot.con.set_joints([0, 0, 0, 0, 0, 0])\n",
    "\t\ttime.sleep(1)\n",
    "\n",
    "\t\t# Debug\n",
    "\t\tprint(\"\\nRobot - Finished picking object!\\n\")\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "\t\"\"\"\n",
    "\tLoads the model state from a checkpoint file.\n",
    "\n",
    "\tParameters:\n",
    "\t\tcheckpoint (dict): The checkpoint containing model state as saved previously.\n",
    "\t\tmodel (torch.nn.Module): The model instance where the state will be loaded.\n",
    "\t\"\"\"\n",
    "\tprint(\"=> Loading checkpoint\")\n",
    "\tmodel.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\t\n",
    "# Postprocessing\n",
    "\n",
    "def get_pieces_features(mask):\n",
    "\t\"\"\"\n",
    "\tGiven a mask path, returns a dictionary with the features of each piece in \n",
    "\tthe image including the radius of the circumcircle.\n",
    "\t\"\"\"\n",
    "\t# Read the image mask\n",
    "\tmask = mask.numpy().squeeze()\n",
    "\tmask = (mask*255).astype(np.uint8)\n",
    "\t# Apply Gaussian blur to reduce noise while preserving edges\n",
    "\t#blur = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "\t\n",
    "\t# Threshold the image to ensure only the pieces are in white\n",
    "\t_, thresh = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\t\n",
    "\t# Find all contours on the thresholded image\n",
    "\tcontours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\t# Filter out very small contours that are likely noise\n",
    "\tcontours = [cnt for cnt in contours if cv2.contourArea(cnt) > 100]\n",
    "\t\n",
    "\t# Initialize list to hold features of each piece\n",
    "\tpieces_features = []\n",
    "\t\n",
    "\t# Process each contour to extract features\n",
    "\tfor piece_contour in contours:\n",
    "\t\t# Create a mask of the piece\n",
    "\t\tpiece_mask = np.zeros_like(mask)\n",
    "\t\tcv2.drawContours(piece_mask, [piece_contour], -1, (255), thickness=cv2.FILLED)\n",
    "\t\t\n",
    "\n",
    "\t\t# Eccentricity (fitEllipse) if the contour has enough points\n",
    "\t\tif piece_contour.shape[0] >= 5:\n",
    "\t\t\t(x, y), (MA, ma), angle = cv2.fitEllipse(piece_contour)\n",
    "\t\t\teccentricity = np.sqrt(1 - (MA / ma) ** 2)\n",
    "\t\telse:\n",
    "\t\t\teccentricity = None\n",
    "\n",
    "\t\t# Centroid and orientation (moments)\n",
    "\t\tM = cv2.moments(piece_contour)\n",
    "\t\tcx = int(M['m10'] / M['m00'])\n",
    "\t\tcy = int(M['m01'] / M['m00'])\n",
    "\t\tcentroid = (cx, cy)\n",
    "\n",
    "\t\t# Circumcircle (minEnclosingCircle)\n",
    "\t\t(x, y), radius = cv2.minEnclosingCircle(piece_contour)\n",
    "\n",
    "\t\t# Compile features into a dictionary\n",
    "\t\tfeatures = {\n",
    "\t\t\t#'aspect_ratio': aspect_ratio,\n",
    "\t\t\t'eccentricity': eccentricity,\n",
    "\t\t\t#'area': area,\n",
    "\t\t\t#'perimeter': perimeter,\n",
    "\t\t\t'centroid': centroid,\n",
    "\t\t\t#'orientation': orientation,\n",
    "\t\t\t#'convexity': convexity,\n",
    "\t\t\t'radius': radius\n",
    "\t\t}\n",
    "\t\t# Add features of the current piece to the list\n",
    "\t\tpieces_features.append(features)\n",
    "\n",
    "\treturn pieces_features\n",
    "\n",
    "def save_annotated_image(image, save_path, pieces_features):\n",
    "\n",
    "\t\"\"\"\n",
    "\tSaves the image to the given path, annotated with the circumcircle and centroid mark for each piece in red for visibility.\n",
    "\tConverts grayscale images to RGB before annotation.\n",
    "\tArgs:\n",
    "\t- image (numpy array): The image array.\n",
    "\t- save_path (str): Path to save the annotated image.\n",
    "\t- pieces_features (list): List of dictionaries containing features of each piece including the radius and centroid.\n",
    "\t\"\"\"\n",
    "\timage = image.numpy().transpose(1, 2, 0)\n",
    "\timage = (image * 255).astype(np.uint8)\n",
    "\t# Convert grayscale image to RGB\n",
    "\tif image.ndim == 2 or (image.ndim == 3 and image.shape[2] == 1):\n",
    "\t\timage = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\t# Annotate each piece\n",
    "\tfor features in pieces_features:\n",
    "\t\t# Draw the circumcircle in red\n",
    "\t\tcv2.circle(image, (int(features['centroid'][0]), int(features['centroid'][1])), int(features['radius']), (0, 0, 255), 2)\n",
    "\t\t# Draw the centroid as a red 'X'\n",
    "\t\tcv2.drawMarker(image, (int(features['centroid'][0]), int(features['centroid'][1])), (0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=10, thickness=2)\n",
    "\t# Save the annotated image\n",
    "\tcv2.imwrite(save_path, image)\n",
    "\n",
    "def stats(data, savedir):\n",
    "\t# Extraer todas las keys posibles (suponiendo que todas las features están en todos los diccionarios)\n",
    "\tkeys = list(data[0][0].keys())\n",
    "\tkeys.remove('centroid')\n",
    "\t#keys.remove('aspect_ratio')\n",
    "\t#keys.remove ('perimeter')\n",
    "\t# Crear un diccionario para acumular los valores de cada feature\n",
    "\tfeatures = {key: [] for key in keys}\n",
    "\t\n",
    "\t# Recorrer cada lista de diccionarios y cada diccionario para acumular los valores de las features\n",
    "\tfor sublist in data:\n",
    "\t\tfor dic in sublist:\n",
    "\t\t\tfor key, value in dic.items():\n",
    "\t\t\t\t# Omitir centroides porque son tuplas y no es trivial calcular media y desviación estándar de tuplas\n",
    "\t\t\t\t#if key != 'centroid'and key!= 'area'and key!= 'perimeter':\n",
    "\t\t\t\tif key != 'centroid' :\n",
    "\t\t\t\t\tfeatures[key].append(value)\n",
    "\t\n",
    "\t# Calcular la media y la desviación estándar para cada feature\n",
    "\tstats_dict = {key: (np.mean(values), np.std(values)) for key, values in features.items()}\n",
    "\t\n",
    "\t# Crear el directorio si no existe\n",
    "\tif not os.path.exists(savedir):\n",
    "\t\tos.makedirs(savedir)\n",
    "\t\n",
    "\t# Guardar el diccionario en un archivo JSON\n",
    "\twith open(os.path.join(savedir, 'feature_stats.json'), 'w') as f:\n",
    "\t\tjson.dump(stats_dict, f, indent=4)\n",
    "\t\n",
    "\treturn stats_dict\n",
    "\n",
    "def load_stats(filepath):\n",
    "\t\"\"\"\n",
    "\tLoad the statistics from a JSON file.\n",
    "\t\"\"\"\n",
    "\twith open(filepath, 'r') as file:\n",
    "\t\tstats = json.load(file)\n",
    "\treturn stats\n",
    "\n",
    "def filter_pieces(stats_data, pieces_list, std=9):\n",
    "\t\"\"\"\n",
    "\tFilter pieces based on the statistical data provided.\n",
    "\tArgs:\n",
    "\tstats_data (dict): A dictionary with keys as properties and values as (mean, sigma).\n",
    "\tpieces_list (list): A list of dictionaries, where each dictionary contains properties of a piece.\n",
    "\tReturns:\n",
    "\tlist: A list of dictionaries, each containing 'centroid' and 'radius' of valid pieces.\n",
    "\t\"\"\"\n",
    "\tvalid_pieces = []\n",
    "\tstats_data= load_stats(stats_data)\n",
    "\t# Iterate over each piece\n",
    "\tfor piece in pieces_list:\n",
    "\t\tvalid = True\n",
    "\t\t# Check each statistical property\n",
    "\t\tfor key, (mean, sigma) in stats_data.items():\n",
    "\t\t\tif key in piece:  # Only check if the key exists in the piece's data\n",
    "\t\t\t\tvalue = piece[key]\n",
    "\t\t\t\tif not (mean - std * sigma <= value <= mean + std * sigma):#+-7\\sigma\n",
    "\t\t\t\t\t#print(f\"Not valid due to {key}\")\n",
    "\t\t\t\t\tvalid = False\n",
    "\t\t\t\t\tbreak\n",
    "\t\tif valid:\n",
    "\t\t\t# If all properties are valid, add the centroid and radius to the valid list\n",
    "\t\t\tvalid_pieces.append({'centroid': piece['centroid'], 'radius': piece['radius']})\n",
    "\treturn valid_pieces\n",
    "\n",
    "def postprocess(tensor_prediction, area_threshold):\n",
    "\t# Convert tensor to numpy array\n",
    "\timage = tensor_prediction.squeeze().cpu().numpy()\n",
    "\t\n",
    "\t# Ensure the image is in 8-bit format\n",
    "\tif image.dtype != np.uint8:\n",
    "\t\timage = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "\t# Handle color conversion if necessary\n",
    "\tif len(image.shape) == 2:  # It's a grayscale image\n",
    "\t\timage_color = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "\telse:\n",
    "\t\timage_color = image  # It's already a BGR image\n",
    "\n",
    "\t# Define the kernel for morphological operations\n",
    "\tkernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "\t# Apply morphological opening and closing\n",
    "\topening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\tclosing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "\t# Convert to binary image for contour detection\n",
    "\t_, binary = cv2.threshold(closing, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\t\n",
    "\t# Find contours\n",
    "\tcontours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\tmask = np.zeros_like(image)\n",
    "\n",
    "\t# Filter and draw contours by area\n",
    "\tfor contour in contours:\n",
    "\t\tif cv2.contourArea(contour) > area_threshold:\n",
    "\t\t\tcv2.drawContours(mask, [contour], -1, (255), thickness=cv2.FILLED)\n",
    "\n",
    "\t# Further morphological cleaning\n",
    "\tsure_bg = cv2.dilate(mask, kernel, iterations=3)\n",
    "\n",
    "\t# Distance transformation for segmentation\n",
    "\tdist_transform = cv2.distanceTransform(sure_bg, cv2.DIST_L2, 5)\n",
    "\t_, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
    "\tsure_fg = np.uint8(sure_fg)\n",
    "\n",
    "\t# Unknown region\n",
    "\tunknown = cv2.subtract(sure_bg, sure_fg)\n",
    "\n",
    "\t# Connected components to separate different objects\n",
    "\t_, markers = cv2.connectedComponents(sure_fg)\n",
    "\tmarkers = markers + 1\n",
    "\tmarkers[unknown == 255] = 0\n",
    "\n",
    "\t# Watershed algorithm to segment connected parts\n",
    "\tcv2.watershed(image_color, markers)\n",
    "\timage_color[markers == -1] = [255, 0, 0]  # Mark boundaries in red\n",
    "\n",
    "\t# Prepare final mask in the same format as input\n",
    "\tfinal_mask = np.zeros_like(image, dtype=np.uint8)\n",
    "\tfinal_mask[markers > 1] = 1\n",
    "\t\n",
    "\t# Convert final mask back to tensor format\n",
    "\tfinal_tensor = torch.from_numpy(final_mask).unsqueeze(0)  # Add batch dimension if necessary\n",
    "\n",
    "\treturn final_tensor\n",
    "\n",
    "def read_labels(labels_path):\n",
    "\twith open(labels_path, 'r') as file:\n",
    "\t\tline = file.readline().strip()\n",
    "\t\tvalues = np.array(list(map(int, line.split())))\n",
    "\treturn values\n",
    "\n",
    "def calculate_corrections(differences):\n",
    "\ttotal_diff_x = sum(diff[0] for diff in differences)\n",
    "\ttotal_diff_y = sum(diff[1] for diff in differences)\n",
    "\tcount = len(differences)\n",
    "\t\n",
    "\tC_x = total_diff_x / count\n",
    "\tC_y = total_diff_y / count\n",
    "\t\n",
    "\treturn C_x, C_y\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "\t\"\"\"\n",
    "\tA module to perform two consecutive convolution operations followed by batch normalization and ReLU activation.\n",
    "\n",
    "\tAttributes:\n",
    "\t\tconv (nn.Sequential): A sequential container of two convolutional blocks.\n",
    "\n",
    "\tParameters:\n",
    "\t\tin_channels (int): Number of input channels.\n",
    "\t\tout_channels (int): Number of output channels.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, in_channels, out_channels):\n",
    "\t\tsuper(DoubleConv, self).__init__()\n",
    "\t\tself.conv = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tDefines the computation performed at every call of the DoubleConv module.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tx (torch.Tensor): The input data.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\ttorch.Tensor: The output data after passing through the convolution blocks.\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.conv(x)\n",
    "class UNET(nn.Module):\n",
    "\t\"\"\"\n",
    "\tU-Net architecture for image segmentation tasks.\n",
    "\n",
    "\tAttributes:\n",
    "\t\tups (nn.ModuleList): List of modules used in the decoder path of U-Net.\n",
    "\t\tdowns (nn.ModuleList): List of modules used in the encoder path of U-Net.\n",
    "\t\tpool (nn.MaxPool2d): Max pooling layer.\n",
    "\t\tbottleneck (DoubleConv): The bottleneck layer of U-Net.\n",
    "\t\tfinal_conv (nn.Conv2d): Final convolutional layer to produce the output segmentation map.\n",
    "\n",
    "\tParameters:\n",
    "\t\tin_channels (int): Number of channels in the input image.\n",
    "\t\tout_channels (int): Number of channels in the output image.\n",
    "\t\tfeatures (List[int]): Number of features in each layer of the network.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
    "\t\tsuper(UNET, self).__init__()\n",
    "\t\tself.ups = nn.ModuleList()\n",
    "\t\tself.downs = nn.ModuleList()\n",
    "\t\tself.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "\t\tfor feature in features:\n",
    "\t\t\tself.downs.append(DoubleConv(in_channels, feature))\n",
    "\t\t\tin_channels = feature\n",
    "\n",
    "\t\tfor feature in reversed(features):\n",
    "\t\t\tself.ups.append(\n",
    "\t\t\t\tnn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n",
    "\t\t\t)\n",
    "\t\t\tself.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "\t\tself.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "\t\tself.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tDefines the forward pass of the U-Net using skip connections and up-sampling.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tx (torch.Tensor): The input tensor for the U-Net model.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\ttorch.Tensor: The output tensor after processing through U-Net.\n",
    "\t\t\"\"\"\n",
    "\t\tskip_connections = []\n",
    "\n",
    "\t\tfor down in self.downs:\n",
    "\t\t\tx = down(x)\n",
    "\t\t\tskip_connections.append(x)\n",
    "\t\t\tx = self.pool(x)\n",
    "\n",
    "\t\tx = self.bottleneck(x)\n",
    "\t\tskip_connections = skip_connections[::-1]\n",
    "\n",
    "\t\tfor idx in range(0, len(self.ups), 2):\n",
    "\t\t\tx = self.ups[idx](x)\n",
    "\t\t\tskip_connection = skip_connections[idx//2]\n",
    "\n",
    "\t\t\tif x.shape != skip_connection.shape:\n",
    "\t\t\t\tx = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "\t\t\tconcat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "\t\t\tx = self.ups[idx+1](concat_skip)\n",
    "\n",
    "\t\treturn self.final_conv(x)\n",
    "\t\n",
    "def configuration_models(): \n",
    "\t # Define the number of input features\n",
    "\tNINPUT = 3\n",
    "\t# Define the number of output features\n",
    "\tNOUTPUT = 3\n",
    "\t# Define the number of neurons in the hidden layers\n",
    "\tNHIDDEN = 25\n",
    "\t# Set the activation function to Tanh\n",
    "\tACTIVATION = nn.Tanh()\n",
    "\n",
    "\t# Model Initialization\n",
    "\tCHECKPOINT_PATH_UNET= \"./inigoaduna/my_checkpoint.pth.tar\"  # Path to the model checkpoint\n",
    "\tCHECKPOINT_PATH_MLP = \"./inigoaduna/mlp_checkpoint.pth.tar\"\n",
    "\tSCALER_X = \"./inigoaduna/scaler_X.pkl\"\n",
    "\tSCALER_Y = \"./inigoaduna/scaler_y.pkl\"\n",
    "\tSTATSDIR = \"./inigoaduna/feature_stats.json\"\n",
    "\tDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Set device to CUDA if available, otherwise use CPU.    \n",
    "\tprint(f'Device= {DEVICE}')\n",
    " \n",
    "\tmodel_mlp = nn.Sequential(\n",
    "\t\tnn.Linear(NINPUT, NHIDDEN),\n",
    "\t\tACTIVATION,\n",
    "\t\tnn.Linear(NHIDDEN, NHIDDEN),\n",
    "\t\tACTIVATION,\n",
    "\t\tnn.Linear(NHIDDEN, NOUTPUT)\n",
    "\t)\n",
    "\tmodel_mlp.load_state_dict(torch.load(CHECKPOINT_PATH_MLP))\n",
    "\n",
    "\n",
    "\t# Model initialization with specified input and output channels\n",
    "\tmodel_unet = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
    "\tload_checkpoint(torch.load(CHECKPOINT_PATH_UNET ), model_unet)\n",
    "\tmodel_unet.eval()  # Set the model to evaluation mode.       \n",
    "\t# Define a function for the thread\n",
    "\tscaler_X_loaded = joblib.load(SCALER_X)\n",
    "\tscaler_y_loaded = joblib.load(SCALER_Y)\n",
    "\treturn scaler_X_loaded, scaler_y_loaded, model_unet, model_mlp, DEVICE,STATSDIR\n",
    "\n",
    " \n",
    "def camera_task(name):\n",
    "\n",
    "\tIMAGE_HEIGHT = 270  \n",
    "\tIMAGE_WIDTH  = 480\n",
    "\tAREA_TRESHOLD = 9000\n",
    "\tNON_DETECTED_PIECES=0\n",
    "\tSIGMA = 3\n",
    "\tpost_predict_resize = A.Resize(height=1080, width=1920, interpolation=1)  \n",
    "\ttest_transforms = A.Compose(\n",
    "\t[\n",
    "\t\tA.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),  # Resize images to defined dimensions.\n",
    "\t\tA.Normalize(\n",
    "\t\t\tmean=[0.0, 0.0, 0.0],  # Normalize images with a mean of 0.\n",
    "\t\t\tstd=[1.0, 1.0, 1.0],    # Standard deviation for normalization.\n",
    "\t\t\tmax_pixel_value=255.0,  # Maximum pixel value in input images.\n",
    "\t\t),\n",
    "\t\tToTensorV2(),  # Convert images to tensor format compatible with PyTorch.\n",
    "\t],\n",
    "\t)\n",
    "\n",
    "\tscaler_x, scaler_y, model_unet, model_mlp, DEVICE, STATSDIR = configuration_models()\n",
    "\t\n",
    "\t# Global\n",
    "\tglobal xyz_base\n",
    "\n",
    "\t# Loop\n",
    "\twhile True:\n",
    "\n",
    "\t\t# Read frame\n",
    "\t\timage, depth_image = cam.read()\n",
    "  \n",
    "\t\t#image = cv2.imread(image)\n",
    "\t\t#depth = np.load(depth_image)\n",
    "\n",
    "\t\t# Undistort image\n",
    "\t\th, w = image.shape[:2]\n",
    "\t\tnewcameramtx, roi = cv2.getOptimalNewCameraMatrix(cam.mtx, cam.dist, (w,h), 1, (w,h))\n",
    "\t\tmapx, mapy = cv2.initUndistortRectifyMap(cam.mtx, cam.dist, None, newcameramtx, (w, h), 5)\n",
    "\t\t#image = cv2.remap(image, mapx, mapy, cv2.INTER_LINEAR)\n",
    "\n",
    "\t\t# Warp image as if the camera took the image from above, perpendicular to the table\n",
    "\t\twarped_image = cv2.warpPerspective(image, M, (np.shape(image)[1], np.shape(image)[0]))\n",
    "\t\timage= Image.fromarray(cv2.cvtColor(warped_image, cv2.COLOR_BGR2RGB))\n",
    "\t\ttransformed = test_transforms(image=np.array(image))\n",
    "\t\timage = transformed[\"image\"].unsqueeze(0).to(DEVICE)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tprediction = model_unet(image)\n",
    "\t\tprediction = torch.sigmoid(prediction)\n",
    "\t\tprediction = (prediction > 0.5).float()\n",
    "\n",
    "    \t# Resize prediction and save\n",
    "\t\tprediction = prediction.squeeze().cpu().numpy()\n",
    "\t\tresized_prediction = post_predict_resize(image=prediction)['image']\n",
    "\t\ttensor_prediction = torch.from_numpy(resized_prediction).unsqueeze(0)\n",
    "\t\n",
    "\t\t#Postprocessing\n",
    "\t\ttensor_prediction = postprocess(tensor_prediction, AREA_TRESHOLD)    \n",
    "    \n",
    "    \t#Get Pieces features\n",
    "\t\tpieces_features = get_pieces_features(tensor_prediction)\n",
    "\t\t#Filter objects not similar to images\n",
    "\t\tpieces_features = filter_pieces(STATSDIR , pieces_features, SIGMA)\n",
    "\t\trobot_locations = []\n",
    "\t\tif len(pieces_features)!=0:\n",
    "\t\t\tfor piece in pieces_features: \n",
    "            \t#Get Data\n",
    "\t\t\t\tcenter = piece['centroid']\n",
    "\t\t\t\tradius = piece['radius']\n",
    "            \t# Transform pixel on warped image back to original image\n",
    "\t\t\t\tnew_pixel = np.dot(np.linalg.inv(M), np.array([[center[0]], [center[1]], [1]]))\n",
    "\t\t\t\tcenter = [int(new_pixel[0][0]/new_pixel[2][0]), int(new_pixel[1][0]/new_pixel[2][0])]\n",
    "            \n",
    "            \t## Get pixel depth \n",
    "\t\t\t\tpixel_depth = depth_image[center[1], center[0]]\n",
    "            \n",
    "\t\t\t\tx_wc = center[0]\n",
    "\t\t\t\ty_wc = center[1]\n",
    "\t\t\t\tz_wc = pixel_depth \n",
    "            \t# Prepare the model for mlp\n",
    "\t\t\t\tinput_mlp_raw = np.array([[x_wc, y_wc, z_wc]])  # Convert it to numpy and add a dimension\n",
    "\t\t\t\tinput_mlp_scaled = scaler_x.transform(input_mlp_raw)  # Apply The scaling\n",
    "\t\t\t\tinput_mlp = torch.tensor(input_mlp_scaled, dtype=torch.float32)  # Convert to tensor\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\toutput_mlp = model_mlp(input_mlp)\n",
    "\t\t\t\tx_pred, y_pred, z_pred = scaler_y.inverse_transform(output_mlp.numpy().reshape(1, -1)).squeeze()\n",
    "\n",
    "\t\t\t\txyz= np.array ([x_pred, y_pred, 177])\n",
    "\t\t\t\trobot_locations.append(list(xyz))\n",
    "\t\t\t\txyz_base = robot_locations\n",
    "\t\t\t\tprint(f'Robot Frame: ({x_pred},{y_pred},{z_pred})')\n",
    "    \n",
    "\t\t\t\t### The code below is to show the results on the screen\n",
    "\n",
    "\n",
    "\t\t\t\t# Display the raw image with bounding box\n",
    "\t\t\t\tfinal_image = cv2.resize(warped_image, (int(1920/2), int(1080/2)))  \n",
    "\t\t\t\tcv2.imshow('frame1', final_image)\n",
    "\t\t\t\tcv2.resizeWindow(\"frame1\", (int(1920/2), int(1080/2)))  \n",
    "\t\t\t\tcv2.moveWindow(\"frame1\", 0, 0)\n",
    "\t\t\t\tif cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "\t\t\t\t\tbreak \n",
    "\n",
    "\n",
    "\t\t\t\t# Display the mask\n",
    "\t\t\t\tfinal_image = cv2.resize(resized_prediction, (int(1920/2), int(1080/2)))  \n",
    "\t\t\t\tcv2.imshow('frame3', final_image)\n",
    "\t\t\t\tcv2.resizeWindow(\"frame3\", (int(1920/2), int(1080/2)))  \n",
    "\t\t\t\tcv2.moveWindow(\"frame3\", int(1920/2), 0)\n",
    "\t\t\t\tif cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "\t\t\t\t\tbreak \n",
    "\n",
    "\t\t\t\t# Display the ldepth color map\n",
    "\t\t\t\tfinal_image = cv2.resize(cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET), (int(1920/2), int(1080/2)))  \n",
    "\t\t\t\tcv2.imshow('frame4', final_image)\n",
    "\t\t\t\tcv2.resizeWindow(\"frame4\", (int(1920/2), int(1080/2)))  \n",
    "\t\t\t\tcv2.moveWindow(\"frame4\", int(1920/2), int(1080/2))\n",
    "\t\t\t\tif cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "\t\t\t\t\tbreak \n",
    "\t\t\t\t\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Create two threads\n",
    "\ttry:\n",
    "\t\t_thread.start_new_thread(camera_task, (\"Thread-1\", ) )\n",
    "\t\t_thread.start_new_thread(robot_task, (\"Thread-2\", ) )\n",
    "\texcept:\n",
    "\t\tprint (\"Error: unable to start thread\")\n",
    "\n",
    "\t# Loop threads\n",
    "\twhile True:\n",
    "\t\tpass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno_thesis=3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
